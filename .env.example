# Production Environment Variables Template for AI Artist Platform (noktvrn_ai_artist)
# Copy this file to .env and fill in your actual credentials.
# DO NOT COMMIT THE .env FILE TO GIT

# --- API Keys ---
SUNO_API_KEY="your_suno_api_key_here"
PEXELS_API_KEY="your_pexels_api_key_here"
PIXABAY_API_KEY="your_pixabay_api_key_here"

# --- LLM API Keys ---
# Add keys for the LLMs you intend to use
DEEPSEEK_API_KEY="your_deepseek_api_key_here"
GEMINI_API_KEY="your_gemini_api_key_here"
GROK_API_KEY="your_grok_api_key_here"
MISTRAL_API_KEY="your_mistral_api_key_here"
# OPENAI_API_KEY="your_openai_api_key_here" # Optional: Uncomment and add if using OpenAI
# ANTHROPIC_API_KEY="your_anthropic_api_key_here" # Optional: Uncomment and add if using Anthropic/Claude models

# --- Telegram Integration ---
# Required for sending previews via Batch Runner and receiving feedback
TELEGRAM_BOT_TOKEN="your_telegram_bot_token_here"
TELEGRAM_CHAT_ID="your_target_telegram_chat_id_here" # The chat ID where the bot sends messages

# --- General Config ---
LOG_LEVEL="INFO" # Optional: DEBUG, INFO, WARNING, ERROR, CRITICAL
OUTPUT_BASE_DIR="/home/ubuntu/ai_artist_system_clone/output" # Example: /path/to/your/output/directory

# --- Batch Runner Config ---
BATCH_POLLING_INTERVAL_SECONDS=60
BATCH_MAX_WAIT_TIME_SECONDS=3600 # 1 hour

# --- Release Chain Config ---
RELEASE_LOG_FILE="/home/ubuntu/ai_artist_system_clone/output/release_log.md"
RELEASE_QUEUE_FILE="/home/ubuntu/ai_artist_system_clone/output/release_queue.json"

# --- Database (If applicable) ---
# DATABASE_URL="postgresql://user:password@host:port/database"

# --- Distribution Credentials (Platform Specific - Add as needed) ---
# DISTROKID_USERNAME="your_distrokid_username"
# DISTROKID_PASSWORD="your_distrokid_password"
# SPOTIFY_CLIENT_ID="your_spotify_client_id"
# SPOTIFY_CLIENT_SECRET="your_spotify_client_secret"



# --- Batch Runner Config (Continued) ---
MAX_APPROVAL_WAIT_TIME=300 # Max seconds to wait for Telegram approval
POLL_INTERVAL=10 # Seconds between checking Telegram approval status
REFLECTION_LLM_PRIMARY="deepseek:deepseek-chat" # Primary LLM for generating reflections
REFLECTION_LLM_FALLBACKS="gemini:gemini-pro" # Comma-separated fallback LLMs for reflections
REFLECTION_MAX_TOKENS=500
REFLECTION_TEMPERATURE=0.6
ARTIST_RETIREMENT_THRESHOLD=5 # Consecutive rejections before retiring an artist
ARTIST_CREATION_PROBABILITY=0.1 # Probability (0.0 to 1.0) of creating a new artist each cycle
AB_TESTING_ENABLED="False" # Enable A/B testing framework in batch runner

# --- Error Analysis Service Config ---
ERROR_ANALYSIS_LLM_PRIMARY="deepseek:deepseek-chat" # Primary LLM for analyzing errors
ERROR_ANALYSIS_LLM_FALLBACKS="gemini:gemini-pro" # Comma-separated fallback LLMs for error analysis
ERROR_ANALYSIS_MAX_TOKENS=1000
ERROR_ANALYSIS_TEMPERATURE=0.5
ENGINEER_LLM_PRIMARY="deepseek:deepseek-coder" # Primary LLM for generating fixes (code-focused model recommended)
ENGINEER_LLM_FALLBACKS="gemini:gemini-pro" # Comma-separated fallback LLMs for fix generation
ENGINEER_MAX_TOKENS=1500
ENGINEER_TEMPERATURE=0.3
AUTO_FIX_ENABLED="False" # Enable attempting to auto-apply suggested patches via 'git apply'
MONITOR_INTERVAL_SECONDS=60 # How often the error analysis service checks the log file
ERROR_CONTEXT_LINES=20 # Number of lines before an error to include in analysis context

